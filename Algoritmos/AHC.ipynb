{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame, permitiendo la detección de tipo más precisa\n",
    "df = pd.read_csv('generadoras_activas_corregido.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import itertools\n",
    "\n",
    "def prediction_strength(train_labels, val_labels, n_clusters):\n",
    "    \"\"\"Calcula el Prediction Strength ajustado para tamaños diferentes de conjuntos\"\"\"\n",
    "    def cluster_membership_matrix(labels, n_clusters):\n",
    "        n_samples = len(labels)\n",
    "        membership = np.zeros((n_samples, n_samples))\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                if labels[i] == labels[j]:\n",
    "                    membership[i, j] = 1\n",
    "        return membership\n",
    "\n",
    "    # Obtener matrices de pertenencia para el entrenamiento y la validación\n",
    "    train_membership_matrix = cluster_membership_matrix(train_labels, n_clusters)\n",
    "    val_membership_matrix = cluster_membership_matrix(val_labels, n_clusters)\n",
    "\n",
    "    total_pairs = 0\n",
    "    consistent_pairs = 0\n",
    "\n",
    "    # Calcular consistencia de pares en entrenamiento y validación\n",
    "    for i in range(min(len(train_labels), len(val_labels))):\n",
    "        for j in range(i + 1, min(len(train_labels), len(val_labels))):\n",
    "            if train_membership_matrix[i, j] == 1:\n",
    "                total_pairs += 1\n",
    "                if val_membership_matrix[i, j] == 1:\n",
    "                    consistent_pairs += 1\n",
    "            elif train_membership_matrix[i, j] == 0:\n",
    "                total_pairs += 1\n",
    "                if val_membership_matrix[i, j] == 0:\n",
    "                    consistent_pairs += 1\n",
    "\n",
    "    return consistent_pairs / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "# Asegúrate de que las fechas en el DataFrame original estén en el formato datetime\n",
    "df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "\n",
    "# Agrupar por fecha y llave, y sumar la columna 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "\n",
    "# Aplanar el MultiIndex después de la pivotación\n",
    "pivot_df.columns = [f'{col.strftime(\"%Y-%m-%d\")}' for col in pivot_df.columns]\n",
    "\n",
    "# Añadir la columna de nombres de generadoras al DataFrame\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Convertir el DataFrame a un formato adecuado para el clustering\n",
    "data = pivot_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalizar los datos con MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data_normalized.reshape((data_normalized.shape[0], data_normalized.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos normalizados en una matriz 2D para linkage\n",
    "data_reshaped = data_normalized.reshape(data_normalized.shape[0], -1)\n",
    "\n",
    "# Separar los datos en entrenamiento (90%) y validación (10%)\n",
    "train_size = int(len(data_reshaped) * 0.9)\n",
    "X_train, X_val = data_reshaped[:train_size], data_reshaped[train_size:]\n",
    "\n",
    "# Probar con varios números de clústeres\n",
    "results = []\n",
    "\n",
    "# Métodos y rangos de número de clústeres\n",
    "methods = ['ward', 'complete']  # Métodos de enlace solicitados\n",
    "n_clusters_range = range(2, 31)  # De 2 a 6\n",
    "\n",
    "# Probar combinaciones de métodos y números de clústeres\n",
    "for method, n_clusters in itertools.product(methods, n_clusters_range):\n",
    "    print(f\"\\nProbando con método: {method.upper()}, n_clusters={n_clusters}\")\n",
    "\n",
    "    # Iniciar cronómetro\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Aplicar AHC con los métodos solicitados (ward y complete)\n",
    "    linked = linkage(X_train, method=method)\n",
    "    labels_train = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Clustering para la validación\n",
    "    linked_val = linkage(X_val, method=method)\n",
    "    labels_val = fcluster(linked_val, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "    # Calcular el Prediction Strength\n",
    "    ps_score = prediction_strength(labels_train, labels_val, n_clusters)\n",
    "\n",
    "    # Detener el cronómetro\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    # Guardar los resultados\n",
    "    result = {\n",
    "        'k': n_clusters,\n",
    "        'Prediction Strength': ps_score,\n",
    "        'Execution Time (s)': execution_time,\n",
    "        'method': method\n",
    "    }\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    # Imprimir los resultados para cada combinación\n",
    "    print(f\"Tiempo de ejecución: {execution_time:.2f} segundos\")\n",
    "    print(f\"Prediction Strength: {ps_score:.2f}\\n\")\n",
    "\n",
    "# Mostrar los resultados finales\n",
    "print(\"\\nResultados finales:\")\n",
    "for result in results:\n",
    "    print(f\"k={result['k']}, method={result['method']} -> Prediction Strength: {result['Prediction Strength']:.2f}, Execution Time: {result['Execution Time (s)']:.2f} segundos\")\n",
    "\n",
    "# Convertir los resultados a DataFrame para graficar\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Graficar los resultados de Prediction Strength para cada método\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method in methods:\n",
    "    method_df = results_df[results_df['method'] == method]\n",
    "    plt.plot(method_df['k'], method_df['Prediction Strength'], label=f'Prediction Strength ({method})', marker='o')\n",
    "\n",
    "plt.xlabel('Número de Clusters (K)')\n",
    "plt.ylabel('Prediction Strength')\n",
    "plt.title('Prediction Strength para métodos Ward y Complete')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graficar los tiempos de ejecución para cada método\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method in methods:\n",
    "    method_df = results_df[results_df['method'] == method]\n",
    "    plt.plot(method_df['k'], method_df['Execution Time (s)'], label=f'Tiempo de ejecución ({method})', marker='o')\n",
    "\n",
    "plt.xlabel('Número de Clusters (K)')\n",
    "plt.ylabel('Tiempo de ejecución (s)')\n",
    "plt.title('Tiempo de ejecución para métodos Ward y Complete')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a35284",
   "metadata": {},
   "source": [
    "# WARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cee3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n",
    "\n",
    "# Agrupar por fecha y llave, y sumar la columna 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "\n",
    "# Convertir las columnas de fecha a datetime\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns)\n",
    "\n",
    "# Añadir la columna de nombres de generadoras al DataFrame\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Muestreo: seleccionar una fracción aleatoria de los datos (por ejemplo, el 10%)\n",
    "sample_df = pivot_df.sample(frac=1, random_state=0)  # Ajusta la fracción según sea necesario\n",
    "\n",
    "# Convertir el DataFrame muestreado a un formato adecuado\n",
    "data = sample_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data.reshape((data.shape[0], data.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos normalizados en una matriz 2D para linkage\n",
    "data_reshaped = data_normalized.reshape(data_normalized.shape[0], -1)\n",
    "\n",
    "# Empezar el cronómetro\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar el algoritmo de agrupamiento jerárquico aglomerativo (AHC) con la métrica Ward\n",
    "linked = linkage(data_reshaped, method='ward')\n",
    "\n",
    "# Obtener etiquetas basadas en el número de clusters (por ejemplo, 6 clusters)\n",
    "n_clusters = 10  # Ajusta el número de clusters según sea necesario\n",
    "labels = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "# Añadir los resultados al DataFrame muestreado\n",
    "sample_df['Cluster'] = labels\n",
    "\n",
    "# Reordenar las columnas para mantener 'LLAVE NOMBRE' al principio\n",
    "sample_df = sample_df[['LLAVE NOMBRE', 'Cluster'] + [col for col in sample_df.columns if col not in ['LLAVE NOMBRE', 'Cluster']]]\n",
    "\n",
    "# Calcular la matriz de distancias euclidianas\n",
    "distances = pairwise_distances(data_reshaped)\n",
    "\n",
    "# Calcular el índice de Silueta usando la métrica euclidiana\n",
    "silhouette_avg = silhouette_score(distances, labels, metric='euclidean')\n",
    "\n",
    "# Detener el cronómetro\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular el tiempo de ejecución\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Tiempo de ejecución: {execution_time:.2f} segundos\")\n",
    "print(f\"Tiempo de ejecución: {execution_time / 60:.2f} minutos\")\n",
    "print(f\"Índice de Silueta: {silhouette_avg:.2f}\")\n",
    "\n",
    "# Limpiar las columnas de `pivot_df` asegurándose de que sean fechas\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns, errors='coerce')\n",
    "\n",
    "# Eliminar columnas que no se pudieron convertir a fechas\n",
    "pivot_df = pivot_df.loc[:, pivot_df.columns.notna()]\n",
    "\n",
    "# Obtener la lista de clusters únicos\n",
    "clusters = sample_df['Cluster'].unique()\n",
    "\n",
    "# Crear un DataFrame con la columna 'LLAVE NOMBRE' y 'TIPO' de df\n",
    "tipo_df = df[['LLAVE NOMBRE', 'TIPO']].drop_duplicates()\n",
    "\n",
    "# Crear un diccionario para almacenar los totales y proporciones de tipos por cluster\n",
    "totales_cluster = {}\n",
    "porcentaje_tipo_total = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada tipo\n",
    "total_tipos_df = tipo_df['TIPO'].value_counts().to_frame(name='Total').reset_index()\n",
    "total_tipos_df.columns = ['TIPO', 'Total']\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener el tipo correspondiente para cada generadora\n",
    "    tipos = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada tipo\n",
    "    tipo_counts = tipos['TIPO'].value_counts()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = tipo_counts.sum()\n",
    "    totales_cluster[cluster] = total_cluster\n",
    "    \n",
    "    # Calcular el porcentaje de cada tipo dentro del cluster y agregar el conteo\n",
    "    tipo_porcentaje_y_cantidad = tipo_counts.to_frame(name='Cantidad')\n",
    "    tipo_porcentaje_y_cantidad['Porcentaje'] = (tipo_counts / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los porcentajes y cantidades en el diccionario\n",
    "    porcentaje_tipo_total[cluster] = tipo_porcentaje_y_cantidad\n",
    "\n",
    "# Mostrar los resultados totales por tipo y por cluster\n",
    "print(\"Proporción total de cada tipo de generadora:\")\n",
    "print(total_tipos_df)\n",
    "print()\n",
    "\n",
    "print(\"Totales, cantidades y porcentajes de cada cluster:\")\n",
    "for cluster, total in totales_cluster.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por tipo en el cluster {cluster}:\")\n",
    "    print(porcentaje_tipo_total[cluster])\n",
    "    print()  # Línea en blanco para mayor legibilidad\n",
    "\n",
    "# Calcular las series más cercanas al centroide para cada tipo dentro de cada cluster\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Filtrar las series temporales de las generadoras en el cluster\n",
    "    cluster_series = pivot_df.loc[generadoras]\n",
    "    \n",
    "    # Obtener los tipos de energía en el cluster\n",
    "    tipos_en_cluster = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Iterar sobre cada tipo de energía en el cluster\n",
    "    for tipo in tipos_en_cluster['TIPO'].unique():\n",
    "        # Filtrar las series temporales del tipo de energía actual\n",
    "        series_tipo = cluster_series.loc[tipos_en_cluster[tipos_en_cluster['TIPO'] == tipo]['LLAVE NOMBRE']]\n",
    "        \n",
    "        if not series_tipo.empty:\n",
    "            # Calcular el centroide (media) del tipo de energía\n",
    "            centroide = series_tipo.mean(axis=0)\n",
    "            \n",
    "            # Calcular la distancia entre cada serie temporal y el centroide usando la distancia euclidiana\n",
    "            distancias = np.array([np.linalg.norm(centroide.values - serie.values) for index, serie in series_tipo.iterrows()]).reshape(-1, 1)\n",
    "            \n",
    "            # Encontrar las 3 series más cercanas al centroide\n",
    "            indices_mas_cercanas = np.argsort(distancias, axis=0)[:3].flatten()\n",
    "            series_representativas = series_tipo.iloc[indices_mas_cercanas]\n",
    "            \n",
    "            # Graficar las 3 series temporales representativas\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            for j, (index, serie) in enumerate(series_representativas.iterrows()):\n",
    "                ax.plot(pivot_df.columns, serie, label=f'Cluster {cluster} - Tipo {tipo} - Serie {j+1}', linestyle='-', marker='o')\n",
    "            \n",
    "            # Configuración del gráfico\n",
    "            ax.set_title(f'Series Temporales Representativas del Cluster {cluster} - Tipo {tipo}')\n",
    "            ax.set_xlabel('Fecha')\n",
    "            ax.set_ylabel('Valor')\n",
    "            ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.legend(loc='upper right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950e69f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "# Agrupar por fecha y llave, y sumar la columna 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "\n",
    "# Convertir las columnas de fecha a datetime\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns)\n",
    "\n",
    "# Añadir la columna de nombres de generadoras al DataFrame\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Muestreo: seleccionar una fracción aleatoria de los datos (por ejemplo, el 10%)\n",
    "sample_df = pivot_df.sample(frac=1, random_state=0)\n",
    "\n",
    "# Convertir el DataFrame muestreado a un formato adecuado\n",
    "data = sample_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data.reshape((data.shape[0], data.shape[1], 1)))\n",
    "data_reshaped = data_normalized.reshape(data_normalized.shape[0], -1)\n",
    "\n",
    "# Empezar el cronómetro\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar el algoritmo de agrupamiento jerárquico aglomerativo (AHC) con la métrica Ward\n",
    "linked = linkage(data_reshaped, method='ward')\n",
    "n_clusters = 10\n",
    "labels = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "# Detener el cronómetro\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular el tiempo de ejecución\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Añadir los resultados al DataFrame muestreado\n",
    "sample_df['Cluster'] = labels\n",
    "\n",
    "# Reordenar las columnas para mantener 'LLAVE NOMBRE' al principio\n",
    "sample_df = sample_df[['LLAVE NOMBRE', 'Cluster'] + [col for col in sample_df.columns if col not in ['LLAVE NOMBRE', 'Cluster']]]\n",
    "\n",
    "# Visualización de los clústeres usando PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels):\n",
    "    plt.scatter(data_pca[labels == cluster, 0], data_pca[labels == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando PCA\")\n",
    "plt.xlabel(\"Componente Principal 1 (Varianza Máxima)\")\n",
    "plt.ylabel(\"Componente Principal 2 (Segunda Mayor Varianza)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualización de los clústeres usando t-SNE\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels):\n",
    "    plt.scatter(data_tsne[labels == cluster, 0], data_tsne[labels == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando t-SNE\")\n",
    "plt.xlabel(\"Componente t-SNE 1\")\n",
    "plt.ylabel(\"Componente t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calcular el índice de Silueta usando la métrica euclidiana\n",
    "distances = pairwise_distances(data_reshaped)\n",
    "silhouette_avg = silhouette_score(distances, labels, metric='euclidean')\n",
    "print(f\"Índice de Silueta: {silhouette_avg:.2f}\")\n",
    "\n",
    "# Mostrar los resultados de totales y porcentajes por tipo de generadora en cada clúster\n",
    "tipo_df = df[['LLAVE NOMBRE', 'TIPO']].drop_duplicates()\n",
    "clusters = sample_df['Cluster'].unique()\n",
    "totales_cluster = {}\n",
    "porcentaje_tipo_total = {}\n",
    "\n",
    "for cluster in clusters:\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    tipos = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    tipo_counts = tipos['TIPO'].value_counts()\n",
    "    total_cluster = tipo_counts.sum()\n",
    "    totales_cluster[cluster] = total_cluster\n",
    "    tipo_porcentaje_y_cantidad = tipo_counts.to_frame(name='Cantidad')\n",
    "    tipo_porcentaje_y_cantidad['Porcentaje'] = (tipo_counts / total_cluster) * 100\n",
    "    porcentaje_tipo_total[cluster] = tipo_porcentaje_y_cantidad\n",
    "\n",
    "print(\"Totales y porcentajes por tipo de generadora en cada clúster:\")\n",
    "for cluster, total in totales_cluster.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por tipo en el cluster {cluster}:\")\n",
    "    print(porcentaje_tipo_total[cluster])\n",
    "    print()\n",
    "\n",
    "# Imprimir el tiempo de ejecución\n",
    "print(f\"Tiempo de ejecución: {execution_time:.2f} segundos\")\n",
    "print(f\"Tiempo de ejecución: {execution_time / 60:.2f} minutos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890bdd46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time\n",
    "\n",
    "# Asegúrate de que las fechas en el DataFrame original estén en el formato datetime\n",
    "df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "\n",
    "# Agrupar por fecha y llave, y sumar la columna 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "\n",
    "# Aplanar el MultiIndex después de la pivotación\n",
    "pivot_df.columns = [f'{col.strftime(\"%Y-%m-%d\")}' for col in pivot_df.columns]\n",
    "\n",
    "# Añadir la columna de nombres de generadoras al DataFrame\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Convertir el DataFrame a un formato adecuado para el clustering\n",
    "data = pivot_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalizar los datos con MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data_normalized.reshape((data_normalized.shape[0], data_normalized.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos normalizados en una matriz 2D para linkage\n",
    "data_reshaped = data_normalized.reshape(data_normalized.shape[0], -1)\n",
    "\n",
    "# Empezar el cronómetro\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar el algoritmo de agrupamiento jerárquico aglomerativo (AHC) con el método de ward\n",
    "linked = linkage(data_reshaped, method='ward')\n",
    "\n",
    "# Detener el cronómetro\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular el tiempo de ejecución\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Obtener etiquetas basadas en 10 clusters\n",
    "labels = fcluster(linked, t=10, criterion='maxclust')\n",
    "\n",
    "# Añadir las etiquetas de los clústeres al DataFrame\n",
    "pivot_df['Cluster'] = labels\n",
    "\n",
    "# Crear un DataFrame con la columna 'LLAVE NOMBRE' y 'TIPO' de df\n",
    "tipo_df = df[['LLAVE NOMBRE', 'TIPO']].drop_duplicates()\n",
    "\n",
    "# Obtener la lista de clusters únicos\n",
    "clusters = pivot_df['Cluster'].unique()\n",
    "\n",
    "# Crear un diccionario para almacenar los totales y proporciones de tipos por cluster\n",
    "totales_cluster = {}\n",
    "porcentaje_tipo_total = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada tipo\n",
    "total_tipos_df = tipo_df['TIPO'].value_counts().to_frame(name='Total').reset_index()\n",
    "total_tipos_df.columns = ['TIPO', 'Total']\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = pivot_df[pivot_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener el tipo correspondiente para cada generadora\n",
    "    tipos = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada tipo\n",
    "    tipo_counts = tipos['TIPO'].value_counts()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = tipo_counts.sum()\n",
    "    totales_cluster[cluster] = total_cluster\n",
    "    \n",
    "    # Calcular el porcentaje de cada tipo dentro del cluster y agregar el conteo\n",
    "    tipo_porcentaje_y_cantidad = tipo_counts.to_frame(name='Cantidad')\n",
    "    tipo_porcentaje_y_cantidad['Porcentaje'] = (tipo_counts / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los porcentajes y cantidades en el diccionario\n",
    "    porcentaje_tipo_total[cluster] = tipo_porcentaje_y_cantidad\n",
    "\n",
    "# Mostrar los resultados totales por tipo y por cluster\n",
    "print(\"Proporción total de cada tipo de generadora:\")\n",
    "print(total_tipos_df)\n",
    "print()\n",
    "\n",
    "print(\"Totales, cantidades y porcentajes de cada cluster:\")\n",
    "for cluster, total in totales_cluster.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por tipo en el cluster {cluster}:\")\n",
    "    print(porcentaje_tipo_total[cluster])\n",
    "    print()  # Línea en blanco para mayor legibilidad\n",
    "\n",
    "# Generar dendrograma\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(linked, labels=pivot_df['LLAVE NOMBRE'].values, leaf_rotation=90)\n",
    "plt.title('Dendrograma de Agrupamiento Jerárquico Aglomerativo (Ward)')\n",
    "plt.xlabel('Generadora')\n",
    "plt.ylabel('Distancia')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--')  # Ajusta este valor si es necesario\n",
    "plt.show()\n",
    "\n",
    "# Imprimir el tiempo de ejecución\n",
    "print(f\"Tiempo de ejecución: {execution_time:.2f} segundos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c054584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con la columna 'LLAVE NOMBRE' y 'REGIÓN' de df\n",
    "region_df = df[['LLAVE NOMBRE', 'REGIÓN']].drop_duplicates()\n",
    "\n",
    "# Crear un diccionario para almacenar los totales y proporciones de regiones por cluster\n",
    "totales_cluster_region = {}\n",
    "porcentaje_region_total = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada región\n",
    "total_regiones_df = region_df['REGIÓN'].value_counts().to_frame(name='Total').reset_index()\n",
    "total_regiones_df.columns = ['REGIÓN', 'Total']\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener la región correspondiente para cada generadora\n",
    "    regiones = region_df[region_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada región\n",
    "    region_counts = regiones['REGIÓN'].value_counts()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = region_counts.sum()\n",
    "    totales_cluster_region[cluster] = total_cluster\n",
    "    \n",
    "    # Calcular el porcentaje de cada región dentro del cluster y agregar el conteo\n",
    "    region_porcentaje_y_cantidad = region_counts.to_frame(name='Cantidad')\n",
    "    region_porcentaje_y_cantidad['Porcentaje'] = (region_counts / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los porcentajes y cantidades en el diccionario\n",
    "    porcentaje_region_total[cluster] = region_porcentaje_y_cantidad\n",
    "\n",
    "# Mostrar los resultados totales por región y por cluster\n",
    "print(\"Proporción total de cada región de generadora:\")\n",
    "print(total_regiones_df)\n",
    "print()\n",
    "\n",
    "print(\"Totales, cantidades y porcentajes de cada cluster por región:\")\n",
    "for cluster, total in totales_cluster_region.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por región en el cluster {cluster}:\")\n",
    "    print(porcentaje_region_total[cluster])\n",
    "    print()  # Línea en blanco para mayor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las columnas 'LLAVE NOMBRE', 'REGIÓN' y 'TIPO' de df, eliminando duplicados\n",
    "region_tipo_df = df[['LLAVE NOMBRE', 'REGIÓN', 'TIPO']].drop_duplicates()\n",
    "\n",
    "# Diccionario para almacenar los totales y proporciones de tipos por región y cluster\n",
    "totales_cluster_region_tipo = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada región y tipo\n",
    "total_regiones_df = region_tipo_df.groupby(['REGIÓN', 'TIPO']).size().to_frame(name='Total').reset_index()\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones por región y tipo\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener la región y tipo correspondiente para cada generadora\n",
    "    regiones_tipos = region_tipo_df[region_tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada combinación de región y tipo\n",
    "    region_tipo_counts = regiones_tipos.groupby(['REGIÓN', 'TIPO']).size().to_frame(name='Cantidad').reset_index()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = region_tipo_counts['Cantidad'].sum()\n",
    "    \n",
    "    # Calcular el porcentaje de cada combinación de región y tipo dentro del cluster\n",
    "    region_tipo_counts['Porcentaje'] = (region_tipo_counts['Cantidad'] / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los resultados en el diccionario\n",
    "    totales_cluster_region_tipo[cluster] = region_tipo_counts\n",
    "\n",
    "# Mostrar los resultados totales por región y tipo en cada cluster\n",
    "print(\"Totales, cantidades y porcentajes de cada combinación de región y tipo por cluster:\")\n",
    "for cluster, data in totales_cluster_region_tipo.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras:\")\n",
    "    print(data)\n",
    "    print()  # Línea en blanco para mayor legibilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcf608",
   "metadata": {},
   "source": [
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n",
    "\n",
    "# Agrupar por fecha y llave, y sumar la columna 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "\n",
    "# Convertir las columnas de fecha a datetime\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns)\n",
    "\n",
    "# Añadir la columna de nombres de generadoras al DataFrame\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Muestreo: seleccionar una fracción aleatoria de los datos (por ejemplo, el 10%)\n",
    "sample_df = pivot_df.sample(frac=1, random_state=0)  # Ajusta la fracción según sea necesario\n",
    "\n",
    "# Convertir el DataFrame muestreado a un formato adecuado\n",
    "data = sample_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data.reshape((data.shape[0], data.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos normalizados en una matriz 2D para linkage\n",
    "data_reshaped = data_normalized.reshape(data_normalized.shape[0], -1)\n",
    "\n",
    "# Empezar el cronómetro\n",
    "start_time = time.time()\n",
    "\n",
    "# Aplicar el algoritmo de agrupamiento jerárquico aglomerativo (AHC) con la métrica Complete\n",
    "linked = linkage(data_reshaped, method='complete')\n",
    "\n",
    "# Obtener etiquetas basadas en el número de clusters (por ejemplo, 10 clusters)\n",
    "n_clusters = 10  # Ajusta el número de clusters según sea necesario\n",
    "labels = fcluster(linked, t=n_clusters, criterion='maxclust')\n",
    "\n",
    "# Añadir los resultados al DataFrame muestreado\n",
    "sample_df['Cluster'] = labels\n",
    "\n",
    "# Reordenar las columnas para mantener 'LLAVE NOMBRE' al principio\n",
    "sample_df = sample_df[['LLAVE NOMBRE', 'Cluster'] + [col for col in sample_df.columns if col not in ['LLAVE NOMBRE', 'Cluster']]]\n",
    "\n",
    "# Calcular la matriz de distancias euclidianas\n",
    "distances = pairwise_distances(data_reshaped)\n",
    "\n",
    "# Calcular el índice de Silueta usando la métrica euclidiana\n",
    "silhouette_avg = silhouette_score(distances, labels, metric='euclidean')\n",
    "\n",
    "# Detener el cronómetro\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular el tiempo de ejecución\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Tiempo de ejecución: {execution_time:.2f} segundos\")\n",
    "print(f\"Tiempo de ejecución: {execution_time / 60:.2f} minutos\")\n",
    "print(f\"Índice de Silueta: {silhouette_avg:.2f}\")\n",
    "\n",
    "# Limpiar las columnas de `pivot_df` asegurándose de que sean fechas\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns, errors='coerce')\n",
    "\n",
    "# Eliminar columnas que no se pudieron convertir a fechas\n",
    "pivot_df = pivot_df.loc[:, pivot_df.columns.notna()]\n",
    "\n",
    "# Obtener la lista de clusters únicos\n",
    "clusters = sample_df['Cluster'].unique()\n",
    "\n",
    "# Crear un DataFrame con la columna 'LLAVE NOMBRE' y 'TIPO' de df\n",
    "tipo_df = df[['LLAVE NOMBRE', 'TIPO']].drop_duplicates()\n",
    "\n",
    "# Crear un diccionario para almacenar los totales y proporciones de tipos por cluster\n",
    "totales_cluster = {}\n",
    "porcentaje_tipo_total = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada tipo\n",
    "total_tipos_df = tipo_df['TIPO'].value_counts().to_frame(name='Total').reset_index()\n",
    "total_tipos_df.columns = ['TIPO', 'Total']\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener el tipo correspondiente para cada generadora\n",
    "    tipos = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada tipo\n",
    "    tipo_counts = tipos['TIPO'].value_counts()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = tipo_counts.sum()\n",
    "    totales_cluster[cluster] = total_cluster\n",
    "    \n",
    "    # Calcular el porcentaje de cada tipo dentro del cluster y agregar el conteo\n",
    "    tipo_porcentaje_y_cantidad = tipo_counts.to_frame(name='Cantidad')\n",
    "    tipo_porcentaje_y_cantidad['Porcentaje'] = (tipo_counts / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los porcentajes y cantidades en el diccionario\n",
    "    porcentaje_tipo_total[cluster] = tipo_porcentaje_y_cantidad\n",
    "\n",
    "# Mostrar los resultados totales por tipo y por cluster\n",
    "print(\"Proporción total de cada tipo de generadora:\")\n",
    "print(total_tipos_df)\n",
    "print()\n",
    "\n",
    "print(\"Totales, cantidades y porcentajes de cada cluster:\")\n",
    "for cluster, total in totales_cluster.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por tipo en el cluster {cluster}:\")\n",
    "    print(porcentaje_tipo_total[cluster])\n",
    "    print()  # Línea en blanco para mayor legibilidad\n",
    "\n",
    "# Calcular las series más cercanas al centroide para cada tipo dentro de cada cluster\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Filtrar las series temporales de las generadoras en el cluster\n",
    "    cluster_series = pivot_df.loc[generadoras]\n",
    "    \n",
    "    # Obtener los tipos de energía en el cluster\n",
    "    tipos_en_cluster = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Iterar sobre cada tipo de energía en el cluster\n",
    "    for tipo in tipos_en_cluster['TIPO'].unique():\n",
    "        # Filtrar las series temporales del tipo de energía actual\n",
    "        series_tipo = cluster_series.loc[tipos_en_cluster[tipos_en_cluster['TIPO'] == tipo]['LLAVE NOMBRE']]\n",
    "        \n",
    "        if not series_tipo.empty:\n",
    "            # Calcular el centroide (media) del tipo de energía\n",
    "            centroide = series_tipo.mean(axis=0)\n",
    "            \n",
    "            # Calcular la distancia entre cada serie temporal y el centroide usando la distancia euclidiana\n",
    "            distancias = np.array([np.linalg.norm(centroide.values - serie.values) for index, serie in series_tipo.iterrows()]).reshape(-1, 1)\n",
    "            \n",
    "            # Encontrar las 3 series más cercanas al centroide\n",
    "            indices_mas_cercanas = np.argsort(distancias, axis=0)[:3].flatten()\n",
    "            series_representativas = series_tipo.iloc[indices_mas_cercanas]\n",
    "            \n",
    "            # Graficar las 3 series temporales representativas\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            for j, (index, serie) in enumerate(series_representativas.iterrows()):\n",
    "                ax.plot(pivot_df.columns, serie, label=f'Cluster {cluster} - Tipo {tipo} - Serie {j+1}', linestyle='-', marker='o')\n",
    "            \n",
    "            # Configuración del gráfico\n",
    "            ax.set_title(f'Series Temporales Representativas del Cluster {cluster} - Tipo {tipo}')\n",
    "            ax.set_xlabel('Fecha')\n",
    "            ax.set_ylabel('Valor')\n",
    "            ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.legend(loc='upper right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con la columna 'LLAVE NOMBRE' y 'REGIÓN' de df\n",
    "region_df = df[['LLAVE NOMBRE', 'REGIÓN']].drop_duplicates()\n",
    "\n",
    "# Crear un diccionario para almacenar los totales y proporciones de regiones por cluster\n",
    "totales_cluster_region = {}\n",
    "porcentaje_region_total = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada región\n",
    "total_regiones_df = region_df['REGIÓN'].value_counts().to_frame(name='Total').reset_index()\n",
    "total_regiones_df.columns = ['REGIÓN', 'Total']\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener la región correspondiente para cada generadora\n",
    "    regiones = region_df[region_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada región\n",
    "    region_counts = regiones['REGIÓN'].value_counts()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = region_counts.sum()\n",
    "    totales_cluster_region[cluster] = total_cluster\n",
    "    \n",
    "    # Calcular el porcentaje de cada región dentro del cluster y agregar el conteo\n",
    "    region_porcentaje_y_cantidad = region_counts.to_frame(name='Cantidad')\n",
    "    region_porcentaje_y_cantidad['Porcentaje'] = (region_counts / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los porcentajes y cantidades en el diccionario\n",
    "    porcentaje_region_total[cluster] = region_porcentaje_y_cantidad\n",
    "\n",
    "# Mostrar los resultados totales por región y por cluster\n",
    "print(\"Proporción total de cada región de generadora:\")\n",
    "print(total_regiones_df)\n",
    "print()\n",
    "\n",
    "print(\"Totales, cantidades y porcentajes de cada cluster por región:\")\n",
    "for cluster, total in totales_cluster_region.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras: {total}\")\n",
    "    print(f\"Cantidad y porcentaje por región en el cluster {cluster}:\")\n",
    "    print(porcentaje_region_total[cluster])\n",
    "    print()  # Línea en blanco para mayor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe946a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las columnas 'LLAVE NOMBRE', 'REGIÓN' y 'TIPO' de df, eliminando duplicados\n",
    "region_tipo_df = df[['LLAVE NOMBRE', 'REGIÓN', 'TIPO']].drop_duplicates()\n",
    "\n",
    "# Diccionario para almacenar los totales y proporciones de tipos por región y cluster\n",
    "totales_cluster_region_tipo = {}\n",
    "\n",
    "# Inicializar un DataFrame para acumular el total de cada región y tipo\n",
    "total_regiones_df = region_tipo_df.groupby(['REGIÓN', 'TIPO']).size().to_frame(name='Total').reset_index()\n",
    "\n",
    "# Iterar sobre cada cluster para calcular los totales y proporciones por región y tipo\n",
    "for cluster in clusters:\n",
    "    # Filtrar los datos del cluster\n",
    "    cluster_data = sample_df[sample_df['Cluster'] == cluster]\n",
    "    \n",
    "    # Obtener la lista de generadoras en el cluster\n",
    "    generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "    \n",
    "    # Obtener la región y tipo correspondiente para cada generadora\n",
    "    regiones_tipos = region_tipo_df[region_tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "    \n",
    "    # Contar las ocurrencias de cada combinación de región y tipo\n",
    "    region_tipo_counts = regiones_tipos.groupby(['REGIÓN', 'TIPO']).size().to_frame(name='Cantidad').reset_index()\n",
    "    \n",
    "    # Guardar el total de generadoras en el cluster\n",
    "    total_cluster = region_tipo_counts['Cantidad'].sum()\n",
    "    \n",
    "    # Calcular el porcentaje de cada combinación de región y tipo dentro del cluster\n",
    "    region_tipo_counts['Porcentaje'] = (region_tipo_counts['Cantidad'] / total_cluster) * 100\n",
    "    \n",
    "    # Almacenar los resultados en el diccionario\n",
    "    totales_cluster_region_tipo[cluster] = region_tipo_counts\n",
    "\n",
    "# Mostrar los resultados totales por región y tipo en cada cluster\n",
    "print(\"Totales, cantidades y porcentajes de cada combinación de región y tipo por cluster:\")\n",
    "for cluster, data in totales_cluster_region_tipo.items():\n",
    "    print(f\"Cluster {cluster} - Total generadoras:\")\n",
    "    print(data)\n",
    "    print()  # Línea en blanco para mayor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d29410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
