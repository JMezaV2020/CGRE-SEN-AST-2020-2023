{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame, permitiendo la detección de tipo más precisa\n",
    "df = pd.read_csv('generadoras_activas_2020-2023', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configurar el dispositivo para usar CUDA si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando el dispositivo: {device}\")\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('generadoras_activas_corregido.csv', low_memory=False)\n",
    "df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "\n",
    "# Agrupar y preparar los datos\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns)\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index\n",
    "\n",
    "# Separar en 90% entrenamiento y 10% validación\n",
    "train_df, val_df = train_test_split(pivot_df, test_size=0.1, random_state=42)\n",
    "train_data = train_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "val_data = val_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalización de series temporales\n",
    "scaler = TimeSeriesScalerMinMax()\n",
    "train_data_normalized = scaler.fit_transform(train_data.reshape((train_data.shape[0], train_data.shape[1], 1)))\n",
    "val_data_normalized = scaler.transform(val_data.reshape((val_data.shape[0], val_data.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos a tensores y cargar al dispositivo\n",
    "train_tensor = torch.Tensor(train_data_normalized).to(device)\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Definir el modelo LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, output_size=1, dropout_rate=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            bidirectional=self.bidirectional, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.dropout(lstm_out[:, -1, :])\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Exploración de hiperparámetros del LSTM\n",
    "hidden_sizes = [32, 64, 128]\n",
    "num_layers_list = [1, 2, 3]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "dropout_rates = [0.2, 0.3]\n",
    "\n",
    "best_model = None\n",
    "best_loss = float('inf')\n",
    "best_params = {}\n",
    "results = []\n",
    "\n",
    "# Medir el tiempo de inicio de la búsqueda de hiperparámetros\n",
    "start_time = time.time()\n",
    "\n",
    "# Probar diferentes combinaciones de hiperparámetros\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layers_list:\n",
    "        for lr in learning_rates:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                model = LSTMModel(input_size=1, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                                  output_size=1, dropout_rate=dropout_rate).to(device)\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                # Entrenamiento\n",
    "                num_epochs = 30\n",
    "                epoch_losses = []\n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_loss = 0.0\n",
    "                    for batch in train_loader:\n",
    "                        inputs = batch[0].to(device)\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, inputs[:, -1, :])\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()\n",
    "                    epoch_loss /= len(train_loader)\n",
    "                    epoch_losses.append(epoch_loss)\n",
    "\n",
    "                # Guardar resultados\n",
    "                results.append({\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'num_layers': num_layers,\n",
    "                    'learning_rate': lr,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'loss': epoch_loss,\n",
    "                    'epoch_losses': epoch_losses  # Guardar la historia de pérdidas\n",
    "                })\n",
    "\n",
    "                # Verificar si es el mejor modelo\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_params = {\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'num_layers': num_layers,\n",
    "                        'learning_rate': lr,\n",
    "                        'dropout_rate': dropout_rate\n",
    "                    }\n",
    "                    best_model = model\n",
    "\n",
    "# Calcular y mostrar el tiempo total de búsqueda de hiperparámetros\n",
    "hyperparameter_search_time = time.time() - start_time\n",
    "print(f\"Tiempo de búsqueda de hiperparámetros: {hyperparameter_search_time:.2f} segundos\")\n",
    "\n",
    "# Ordenar resultados por pérdida final y seleccionar los 5 mejores\n",
    "top_results = sorted(results, key=lambda x: x['loss'])[:5]\n",
    "\n",
    "# Graficar los mejores 5 conjuntos de hiperparámetros\n",
    "plt.figure(figsize=(10, 6))\n",
    "for result in top_results:\n",
    "    epoch_losses = result['epoch_losses']\n",
    "    label = f\"HS={result['hidden_size']}, NL={result['num_layers']}, LR={result['learning_rate']}, DR={result['dropout_rate']}\"\n",
    "    plt.plot(epoch_losses, label=label)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comportamiento del LSTM')\n",
    "# Configurar el eje X para mostrar solo números enteros\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mejores Hiperparámetros:\", best_params)\n",
    "\n",
    "# Obtener representaciones comprimidas del mejor modelo LSTM\n",
    "with torch.no_grad():\n",
    "    train_encoded = best_model(train_tensor).cpu().numpy()\n",
    "    val_encoded = best_model(torch.Tensor(val_data_normalized).to(device)).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c559cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular el Prediction Strength\n",
    "def prediction_strength(train_labels, val_labels, n_clusters):\n",
    "    def cluster_membership_matrix(labels, n_clusters):\n",
    "        n_samples = len(labels)\n",
    "        membership = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                if labels[i] == labels[j]:\n",
    "                    membership[i, j] = 1\n",
    "        return membership\n",
    "\n",
    "    train_membership_matrix = cluster_membership_matrix(train_labels, n_clusters)\n",
    "    val_membership_matrix = cluster_membership_matrix(val_labels, n_clusters)\n",
    "\n",
    "    total_pairs = 0\n",
    "    consistent_pairs = 0\n",
    "    for i in range(min(len(train_labels), len(val_labels))):\n",
    "        for j in range(i + 1, min(len(train_labels), len(val_labels))):\n",
    "            if train_membership_matrix[i, j] == 1:\n",
    "                total_pairs += 1\n",
    "                if val_membership_matrix[i, j] == 1:\n",
    "                    consistent_pairs += 1\n",
    "            elif train_membership_matrix[i, j] == 0:\n",
    "                total_pairs += 1\n",
    "                if val_membership_matrix[i, j] == 0:\n",
    "                    consistent_pairs += 1\n",
    "    return consistent_pairs / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "# Obtener representaciones comprimidas del mejor modelo LSTM\n",
    "with torch.no_grad():\n",
    "    train_encoded = best_model(train_tensor).cpu().numpy()\n",
    "    val_encoded = best_model(torch.Tensor(val_data_normalized).to(device)).cpu().numpy()\n",
    "\n",
    "# Prediction Strength para diferentes valores de clusters usando los mejores hiperparámetros\n",
    "n_clusters_range = range(2, 21)\n",
    "results_euclidean = []\n",
    "results_dtw = []\n",
    "\n",
    "for metric in ['euclidean', 'dtw']:\n",
    "    print(f\"\\nProbando clustering con métrica: {metric.upper()}\")\n",
    "    for n_clusters in n_clusters_range:\n",
    "        start_time = time.time()\n",
    "        model = TimeSeriesKMeans(n_clusters=n_clusters, metric=metric, random_state=0)\n",
    "        train_labels = model.fit_predict(train_encoded)\n",
    "        val_labels = model.predict(val_encoded)\n",
    "        ps_score = prediction_strength(train_labels, val_labels, n_clusters)\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"n_clusters = {n_clusters}, Prediction Strength = {ps_score:.2f}, Tiempo de ejecución = {execution_time:.2f} s\")\n",
    "\n",
    "        result = {\n",
    "            'k': n_clusters,\n",
    "            'Prediction Strength': ps_score,\n",
    "            'Execution Time (s)': execution_time\n",
    "        }\n",
    "        if metric == 'euclidean':\n",
    "            results_euclidean.append(result)\n",
    "        else:\n",
    "            results_dtw.append(result)\n",
    "\n",
    "# Convertir resultados a DataFrame para graficar\n",
    "df_euclidean = pd.DataFrame(results_euclidean)\n",
    "df_dtw = pd.DataFrame(results_dtw)\n",
    "\n",
    "# Graficar resultados para Euclidean\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_euclidean['k'], df_euclidean['Prediction Strength'], label='Prediction Strength (Euclidean)', marker='o')\n",
    "# Configurar el eje X para mostrar solo números enteros\n",
    "plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "plt.xlabel('Número de Clusters (K)')\n",
    "plt.ylabel('Prediction Strength')\n",
    "plt.title('Euclidean: Prediction Strength vs Número de Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Graficar resultados para DTW si hay datos\n",
    "if not df_dtw.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df_dtw['k'], df_dtw['Prediction Strength'], label='Prediction Strength (DTW)', marker='o')\n",
    "    # Configurar el eje X para mostrar solo números enteros\n",
    "    plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    plt.xlabel('Número de Clusters (K)')\n",
    "    plt.ylabel('Prediction Strength')\n",
    "    plt.title('DTW: Prediction Strength vs Número de Clusters')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay resultados para DTW.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20252e24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.cluster import KMeans\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.metrics import cdist_dtw\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configurar el dispositivo para usar CUDA si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando el dispositivo: {device}\")\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('generadoras_activas_corregido.csv', low_memory=False)\n",
    "df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "\n",
    "# Agrupar por fecha y llave, sumar 'TOTAL'\n",
    "energia_por_fecha = df.groupby(['LLAVE NOMBRE', 'FECHA'])['TOTAL'].sum().reset_index()\n",
    "\n",
    "# Pivotar los datos para tener fechas como columnas y llaves como filas\n",
    "pivot_df = energia_por_fecha.pivot(index='LLAVE NOMBRE', columns='FECHA', values='TOTAL').fillna(0)\n",
    "pivot_df.columns = pd.to_datetime(pivot_df.columns)\n",
    "pivot_df['LLAVE NOMBRE'] = pivot_df.index  # Añadir columna con nombres de generadoras\n",
    "\n",
    "# Usar todos los datos sin muestreo\n",
    "data = pivot_df.drop(columns=['LLAVE NOMBRE']).values\n",
    "\n",
    "# Normalización específica de series temporales\n",
    "data_normalized = TimeSeriesScalerMinMax().fit_transform(data.reshape((data.shape[0], data.shape[1], 1)))\n",
    "\n",
    "# Convertir los datos a tensores y cargar al dispositivo\n",
    "data_tensor = torch.Tensor(data_normalized).to(device)\n",
    "train_dataset = TensorDataset(data_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Definir el modelo Autoencoder LSTM\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, encoded_size=32, num_layers=2, dropout_rate=0.3):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, num_layers=num_layers, \n",
    "                               bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.hidden_to_encoded = nn.Linear(hidden_size * 2, encoded_size)  # Capa para codificación\n",
    "\n",
    "        self.encoded_to_hidden = nn.Linear(encoded_size, hidden_size * 2)  # Ajuste de tamaño para el decodificador\n",
    "        self.decoder = nn.LSTM(hidden_size * 2, hidden_size, num_layers=num_layers, \n",
    "                               bidirectional=True, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size * 2, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        batch_size = x.size(0)\n",
    "        h_0 = torch.randn(2 * self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        c_0 = torch.randn(2 * self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        encoder_out, _ = self.encoder(x, (h_0, c_0))\n",
    "        encoder_out = self.dropout(encoder_out[:, -1, :])  # Toma la última salida del encoder\n",
    "        encoded = self.hidden_to_encoded(encoder_out)  # Codificación de la serie temporal\n",
    "\n",
    "        # Decoder\n",
    "        decoder_hidden = self.encoded_to_hidden(encoded).view(2 * self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0_dec = torch.randn(2 * self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        decoder_input = self.encoded_to_hidden(encoded).unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        \n",
    "        # Ejecutar el decodificador\n",
    "        decoder_out, _ = self.decoder(decoder_input, (decoder_hidden, c_0_dec))\n",
    "        out = self.output_layer(decoder_out)\n",
    "\n",
    "        return out, encoded  # Devuelve tanto la reconstrucción como la codificación\n",
    "\n",
    "# Configuración y entrenamiento del modelo Autoencoder LSTM\n",
    "encoded_size = 32\n",
    "model = LSTMAutoencoder(input_size=1, hidden_size=128, encoded_size=encoded_size, num_layers=1, dropout_rate=0.3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, encoded = model(inputs)  # obtener salida y codificación\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Tiempo de entrenamiento del Autoencoder LSTM: {training_time:.2f} segundos\")\n",
    "\n",
    "# Fragmentación para evitar problemas de memoria y obtener las representaciones codificadas\n",
    "with torch.no_grad():\n",
    "    batch_size = 128  # Ajusta este valor según la capacidad de memoria\n",
    "    encoded_data_total = []\n",
    "    for i in range(0, data_tensor.size(0), batch_size):\n",
    "        batch_data = data_tensor[i:i + batch_size]\n",
    "        _, encoded_batch = model(batch_data)  # Solo obtenemos la codificación\n",
    "        encoded_data_total.append(encoded_batch.cpu().numpy())\n",
    "    encoded_data_total = np.concatenate(encoded_data_total, axis=0)\n",
    "\n",
    "# Clustering usando KMeans (Euclidiana)\n",
    "start_kmeans_time = time.time()\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "labels_kmeans = kmeans.fit_predict(encoded_data_total)\n",
    "kmeans_time = time.time() - start_kmeans_time\n",
    "print(f\"Tiempo de ejecución para KMeans (Euclidiana): {kmeans_time:.2f} segundos\")\n",
    "\n",
    "# Clustering usando DTW\n",
    "start_dtw_time = time.time()\n",
    "model_dtw = TimeSeriesKMeans(n_clusters=n_clusters, metric=\"dtw\", random_state=0)\n",
    "labels_dtw = model_dtw.fit_predict(encoded_data_total)\n",
    "dtw_time = time.time() - start_dtw_time\n",
    "print(f\"Tiempo de ejecución para KMeans (DTW): {dtw_time:.2f} segundos\")\n",
    "\n",
    "# Calcular la matriz de distancias DTW y el índice de silueta para DTW\n",
    "dist_matrix_dtw = cdist_dtw(encoded_data_total)\n",
    "np.fill_diagonal(dist_matrix_dtw, 0)\n",
    "silhouette_avg_dtw = silhouette_score(dist_matrix_dtw, labels_dtw, metric=\"precomputed\")\n",
    "print(f\"Índice de Silueta con DTW: {silhouette_avg_dtw:.2f}\")\n",
    "\n",
    "# Calcular el índice de silueta para KMeans (Euclidiana)\n",
    "silhouette_avg_kmeans = silhouette_score(encoded_data_total, labels_kmeans)\n",
    "print(f\"Índice de Silueta con KMeans (Euclidiana): {silhouette_avg_kmeans:.2f}\")\n",
    "\n",
    "# Añadir etiquetas de cluster al DataFrame para KMeans y DTW\n",
    "pivot_df['Cluster_KMeans'] = labels_kmeans\n",
    "pivot_df['Cluster_DTW'] = labels_dtw\n",
    "\n",
    "# Mostrar proporciones para cada tipo de generadora en cada cluster\n",
    "tipo_df = df[['LLAVE NOMBRE', 'TIPO']].drop_duplicates()\n",
    "\n",
    "def calcular_proporciones(cluster_labels, metodo):\n",
    "    clusters = np.unique(cluster_labels)\n",
    "    totales_cluster = {}\n",
    "    porcentaje_tipo_total = {}\n",
    "    total_tipos_df = tipo_df['TIPO'].value_counts().to_frame(name='Total').reset_index()\n",
    "    total_tipos_df.columns = ['TIPO', 'Total']\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_data = pivot_df[pivot_df[f'Cluster_{metodo}'] == cluster]\n",
    "        generadoras = cluster_data['LLAVE NOMBRE'].unique()\n",
    "        tipos = tipo_df[tipo_df['LLAVE NOMBRE'].isin(generadoras)]\n",
    "        tipo_counts = tipos['TIPO'].value_counts()\n",
    "        total_cluster = tipo_counts.sum()\n",
    "        totales_cluster[cluster] = total_cluster\n",
    "        tipo_porcentaje_y_cantidad = tipo_counts.to_frame(name='Cantidad')\n",
    "        tipo_porcentaje_y_cantidad['Porcentaje'] = (tipo_counts / total_cluster) * 100\n",
    "        porcentaje_tipo_total[cluster] = tipo_porcentaje_y_cantidad\n",
    "\n",
    "    print(f\"\\nProporción total de cada tipo de generadora para {metodo}:\")\n",
    "    print(total_tipos_df)\n",
    "    for cluster, total in totales_cluster.items():\n",
    "        print(f\"\\nCluster {cluster} - Total generadoras: {total}\")\n",
    "        print(porcentaje_tipo_total[cluster])\n",
    "\n",
    "# Mostrar proporciones para KMeans y DTW\n",
    "calcular_proporciones(labels_kmeans, 'KMeans')\n",
    "calcular_proporciones(labels_dtw, 'DTW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519039e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Análisis con PCA\n",
    "pca = PCA(n_components=2)\n",
    "encoded_pca = pca.fit_transform(encoded_data_total)\n",
    "\n",
    "# Visualización de PCA para KMeans\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels_kmeans):\n",
    "    plt.scatter(encoded_pca[labels_kmeans == cluster, 0], encoded_pca[labels_kmeans == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando PCA\")\n",
    "plt.xlabel(\"Componente Principal 1 (Varianza Máxima)\")\n",
    "plt.ylabel(\"Componente Principal 2 (Segunda Mayor Varianza)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualización de PCA para DTW\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels_dtw):\n",
    "    plt.scatter(encoded_pca[labels_dtw == cluster, 0], encoded_pca[labels_dtw == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando PCA\")\n",
    "plt.xlabel(\"Componente Principal 1 (Varianza Máxima)\")\n",
    "plt.ylabel(\"Componente Principal 2 (Segunda Mayor Varianza)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Análisis con t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=0)\n",
    "encoded_tsne = tsne.fit_transform(encoded_data_total)\n",
    "\n",
    "# Visualización de t-SNE para KMeans\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels_kmeans):\n",
    "    plt.scatter(encoded_tsne[labels_kmeans == cluster, 0], encoded_tsne[labels_kmeans == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando t-SNE\")\n",
    "plt.xlabel(\"Componente t-SNE 1\")\n",
    "plt.ylabel(\"Componente t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualización de t-SNE para DTW\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in np.unique(labels_dtw):\n",
    "    plt.scatter(encoded_tsne[labels_dtw == cluster, 0], encoded_tsne[labels_dtw == cluster, 1], label=f'Cluster {cluster}')\n",
    "plt.title(\"Visualización de Clústeres usando t-SNE\")\n",
    "plt.xlabel(\"Componente t-SNE 1\")\n",
    "plt.ylabel(\"Componente t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e83bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
